{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'BikeAcademy' started successfully!\n",
      "Unity Academy name: BikeAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: AIBike\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 31\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [5]\n",
      "        Vector Action descriptions: , , , , \n",
      "INFO:gym_unity:11 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "# Name of the Unity environment binary to launch\n",
    "multi_env_name = \"D:/AI/rl/BikeScene2/Bike\"  \n",
    "multi_env = UnityEnv(multi_env_name, worker_id=1, \n",
    "                     use_visual=False, multiagent=True)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(multi_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent observations look like: \n",
      "[-1.65771483e-03 -4.93333326e-04  4.93624687e-01 -1.88574218e-03\n",
      "  2.05333321e-03 -9.86403704e-01  0.00000000e+00  0.00000000e+00\n",
      "  3.13759327e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  1.33482754e-01\n",
      "  9.91051137e-01  0.00000000e+00  7.26167411e-02  0.00000000e+00\n",
      "  6.25000000e-02  5.00000000e-01  5.00000000e-01  7.35828642e-09\n",
      " -3.20334443e-08  8.39731842e-02  9.96468127e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.00000000e-01]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "initial_observations = multi_env.reset()\n",
    "\n",
    "if len(multi_env.observation_space.shape) == 1:\n",
    "    # Examine the initial vector observation\n",
    "    print(\"Agent observations look like: \\n{}\".format(initial_observations[0]))\n",
    "else:\n",
    "    # Examine the initial visual observation\n",
    "    print(\"Agent observations look like:\")\n",
    "    if multi_env.observation_space.shape[2] == 3:\n",
    "        plt.imshow(initial_observations[0][:,:,:])\n",
    "    else:\n",
    "        plt.imshow(initial_observations[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: 0.7880495712161063\n",
      "Total reward this episode: 1.675109875473109\n",
      "Total reward this episode: 1.5428011905063281\n",
      "Total reward this episode: 1.5572899837385523\n",
      "Total reward this episode: 2.056932994587855\n",
      "Total reward this episode: 2.0249131044203588\n",
      "Total reward this episode: 1.5389903133565728\n",
      "Total reward this episode: 1.4168540212241085\n",
      "Total reward this episode: 0.8026280525055804\n",
      "Total reward this episode: 0.838215595619245\n",
      "Total reward this episode: 0.6993689984083175\n",
      "Total reward this episode: 1.8580805103887208\n",
      "Total reward this episode: 1.8199705373157153\n",
      "Total reward this episode: 0.8436403559012844\n",
      "Total reward this episode: 1.7800636013800446\n",
      "Total reward this episode: 0.5117421312765641\n",
      "Total reward this episode: 1.9715240340341218\n",
      "Total reward this episode: 1.2804639136249372\n",
      "Total reward this episode: 1.37995887615464\n",
      "Total reward this episode: 1.034510947086594\n",
      "Total reward this episode: 1.5971817320043387\n",
      "Total reward this episode: 1.4273623187433588\n",
      "Total reward this episode: 0.03941439600153404\n",
      "Total reward this episode: 1.3038261316039352\n",
      "Total reward this episode: 1.1845767213539644\n",
      "Total reward this episode: 0.23227130824869308\n",
      "Total reward this episode: 1.7986685206944288\n",
      "Total reward this episode: 1.7156433659521015\n",
      "Total reward this episode: 1.1835709634152323\n",
      "Total reward this episode: 0.8869000043381339\n",
      "Total reward this episode: 1.5826015567237681\n",
      "Total reward this episode: 1.6509408537637102\n",
      "Total reward this episode: 1.2347761175849221\n",
      "Total reward this episode: 2.945809633217074\n",
      "Total reward this episode: 0.7667033889076927\n",
      "Total reward this episode: 0.9986469623717394\n",
      "Total reward this episode: 1.8683874749324538\n",
      "Total reward this episode: 0.9499518735842271\n",
      "Total reward this episode: 1.6062798899683082\n",
      "Total reward this episode: 1.8429022350094533\n",
      "Total reward this episode: 1.0793075764721092\n",
      "Total reward this episode: 1.2261679633097213\n",
      "Total reward this episode: 0.7644430033185261\n",
      "Total reward this episode: -0.061625063419342374\n",
      "Total reward this episode: 0.9695108824155543\n",
      "Total reward this episode: 0.8916547162966293\n",
      "Total reward this episode: 0.5467712682756508\n",
      "Total reward this episode: 1.5959139317274094\n",
      "Total reward this episode: 1.2421248812567105\n",
      "Total reward this episode: 2.030204859646884\n",
      "Total reward this episode: 1.3989113731817762\n",
      "Total reward this episode: 2.0020813765850933\n",
      "Total reward this episode: 1.1184118078513579\n",
      "Total reward this episode: 1.4196718707680702\n",
      "Total reward this episode: 2.157455794513225\n",
      "Total reward this episode: 1.4169266765767876\n",
      "Total reward this episode: 1.5400746640833942\n",
      "Total reward this episode: 1.724788959730755\n",
      "Total reward this episode: 1.8267829871990464\n",
      "Total reward this episode: 1.7868507748300377\n",
      "Total reward this episode: 0.7855991748246278\n",
      "Total reward this episode: 1.2929077317768876\n",
      "Total reward this episode: 1.9695757979696447\n",
      "Total reward this episode: -0.12512164867737058\n",
      "Total reward this episode: 0.021368456157770943\n",
      "Total reward this episode: 0.40443511104041874\n",
      "Total reward this episode: 2.280507024038922\n",
      "Total reward this episode: 1.9553970965472132\n",
      "Total reward this episode: 1.7272234924814915\n",
      "Total reward this episode: 1.5596031194383448\n",
      "Total reward this episode: 0.7403925318609585\n",
      "Total reward this episode: 1.9984465674920517\n",
      "Total reward this episode: 1.1497325511141259\n",
      "Total reward this episode: 0.6188680583780459\n",
      "Total reward this episode: 0.16775494339791208\n",
      "Total reward this episode: 1.5557196871800858\n",
      "Total reward this episode: 1.820736294442957\n",
      "Total reward this episode: 1.8679672777652736\n",
      "Total reward this episode: 1.5413015132600612\n",
      "Total reward this episode: 1.6824584176594561\n",
      "Total reward this episode: 0.18783055110411218\n",
      "Total reward this episode: 1.1856151290915227\n",
      "Total reward this episode: 1.07561144368215\n",
      "Total reward this episode: 0.020113176920197397\n",
      "Total reward this episode: 1.8477438674731688\n",
      "Total reward this episode: 2.2372445728291166\n",
      "Total reward this episode: 0.7462657608769157\n",
      "Total reward this episode: 1.8717847689986231\n",
      "Total reward this episode: 1.742556314576756\n",
      "Total reward this episode: 1.1484982445836065\n",
      "Total reward this episode: 2.149921945550225\n",
      "Total reward this episode: 1.1881990080529992\n",
      "Total reward this episode: 1.3345612978393382\n",
      "Total reward this episode: 0.1979979242790832\n",
      "Total reward this episode: 0.8075220882892604\n",
      "Total reward this episode: 0.785158342935822\n",
      "Total reward this episode: 1.5429458049210643\n",
      "Total reward this episode: 1.2391296882521021\n",
      "Total reward this episode: 1.2569152834740551\n",
      "Total reward this episode: 1.9166388985785572\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    initial_observation = multi_env.reset()\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        multi_env.render()\n",
    "        actions = [multi_env.action_space.sample() for agent in range(multi_env.number_agents)]\n",
    "        observations, rewards, dones, info = multi_env.step(actions)\n",
    "        episode_rewards += np.mean(rewards)\n",
    "        done = dones[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
