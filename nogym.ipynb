{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = r\"D:\\AI\\rl\\BikeScene\\Bike\"  # Name of the Unity environment binary to launch\n",
    "train_mode = True  # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'BikeAcademy' started successfully!\n",
      "Unity Academy name: BikeAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: AIBike\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 31\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [5]\n",
      "        Vector Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  7.85725933e-05\n",
      "  2.05333321e-03 -9.85398114e-01  0.00000000e+00  0.00000000e+00\n",
      "  2.54820406e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00 -1.67399719e-01\n",
      "  9.85889137e-01  0.00000000e+00 -2.44330969e-02  0.00000000e+00\n",
      "  6.25000000e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    #显示总共的31个状态。\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -1.348052203655243\n",
      "Total reward this episode: -1.568811222910881\n",
      "Total reward this episode: 1.7913934886455536\n",
      "Total reward this episode: 2.0734746754169464\n",
      "Total reward this episode: -2.6527199447155\n",
      "Total reward this episode: 3.124478429555893\n",
      "Total reward this episode: -1.4300427734851837\n",
      "Total reward this episode: -1.5465737283229828\n",
      "Total reward this episode: 5.645796284079552\n",
      "Total reward this episode: -3.0953075513243675\n",
      "Total reward this episode: 1.6593569815158844\n",
      "Total reward this episode: -1.034256398677826\n",
      "Total reward this episode: 1.1885029897093773\n",
      "Total reward this episode: 0.8464796841144562\n",
      "Total reward this episode: 1.4814735651016235\n",
      "Total reward this episode: 0.1395314335823059\n",
      "Total reward this episode: 1.3610121011734009\n",
      "Total reward this episode: 1.4820502698421478\n",
      "Total reward this episode: -2.447913847863674\n",
      "Total reward this episode: -0.715921550989151\n",
      "Total reward this episode: -0.6556797623634338\n",
      "Total reward this episode: 1.929302155971527\n",
      "Total reward this episode: -1.907330721616745\n",
      "Total reward this episode: 1.8321372866630554\n",
      "Total reward this episode: 1.1877484619617462\n",
      "Total reward this episode: 0.11293178796768188\n",
      "Total reward this episode: -2.238283723592758\n",
      "Total reward this episode: -2.133000612258911\n",
      "Total reward this episode: -1.650067150592804\n",
      "Total reward this episode: 3.7193039059638977\n",
      "Total reward this episode: 1.1189046800136566\n",
      "Total reward this episode: 1.8461685478687286\n",
      "Total reward this episode: 1.0043279826641083\n",
      "Total reward this episode: 0.21339592337608337\n",
      "Total reward this episode: 0.1262563318014145\n",
      "Total reward this episode: -0.5153200626373291\n",
      "Total reward this episode: 2.636960744857788\n",
      "Total reward this episode: 1.5881509482860565\n",
      "Total reward this episode: -2.2741259336471558\n",
      "Total reward this episode: -2.2167966663837433\n",
      "Total reward this episode: 2.811186760663986\n",
      "Total reward this episode: -0.08659949898719788\n",
      "Total reward this episode: 2.4898606836795807\n",
      "Total reward this episode: 0.24976953864097595\n",
      "Total reward this episode: -0.600656270980835\n",
      "Total reward this episode: -2.3864307701587677\n",
      "Total reward this episode: 1.6238511502742767\n",
      "Total reward this episode: 1.0287012159824371\n",
      "Total reward this episode: 2.0545257925987244\n",
      "Total reward this episode: 0.09643781185150146\n",
      "Total reward this episode: -1.6260727643966675\n",
      "Total reward this episode: 1.4959654062986374\n",
      "Total reward this episode: 0.46018557250499725\n",
      "Total reward this episode: -3.0643928796052933\n",
      "Total reward this episode: 0.4498786926269531\n",
      "Total reward this episode: 1.785102128982544\n",
      "Total reward this episode: -0.9465798735618591\n",
      "Total reward this episode: 0.9289356470108032\n",
      "Total reward this episode: 0.029464900493621826\n",
      "Total reward this episode: 0.5978009253740311\n",
      "Total reward this episode: -0.5297204703092575\n",
      "Total reward this episode: 0.6988748908042908\n",
      "Total reward this episode: -1.224936157464981\n",
      "Total reward this episode: 1.5497119277715683\n",
      "Total reward this episode: -0.9626372829079628\n",
      "Total reward this episode: -2.4984438493847847\n",
      "Total reward this episode: 2.497369460761547\n",
      "Total reward this episode: -1.1269648373126984\n",
      "Total reward this episode: 0.9108839631080627\n",
      "Total reward this episode: -0.6114352494478226\n",
      "Total reward this episode: -0.1311463713645935\n",
      "Total reward this episode: 1.0033444166183472\n",
      "Total reward this episode: -0.04600699245929718\n",
      "Total reward this episode: -0.7016674280166626\n",
      "Total reward this episode: -1.750261902809143\n",
      "Total reward this episode: 1.7135565876960754\n",
      "Total reward this episode: 3.2088526785373688\n",
      "Total reward this episode: 1.070386826992035\n",
      "Total reward this episode: 2.793578028678894\n",
      "Total reward this episode: -0.8791752308607101\n",
      "Total reward this episode: -1.0531810224056244\n",
      "Total reward this episode: -0.18598097562789917\n",
      "Total reward this episode: -1.8912237212061882\n",
      "Total reward this episode: -0.5304096043109894\n",
      "Total reward this episode: -2.7544125467538834\n",
      "Total reward this episode: -1.8060663938522339\n",
      "Total reward this episode: 0.5911579430103302\n",
      "Total reward this episode: 0.38740693032741547\n",
      "Total reward this episode: 2.1510081440210342\n",
      "Total reward this episode: 4.8032446205616\n",
      "Total reward this episode: 1.3850409537553787\n",
      "Total reward this episode: 1.1524540483951569\n",
      "Total reward this episode: 1.0504269897937775\n",
      "Total reward this episode: 3.614995837211609\n",
      "Total reward this episode: -1.1234052628278732\n",
      "Total reward this episode: -0.7089182883501053\n",
      "Total reward this episode: -1.6616479195654392\n",
      "Total reward this episode: -0.44024357199668884\n",
      "Total reward this episode: 0.5620485693216324\n",
      "Total reward this episode: 1.8090801239013672\n"
     ]
    }
   ],
   "source": [
    "#运行完如果你看画面不动，那是因为episode就100，已经运行完了。真正训练时你把episode设置的数值很大的。\n",
    "#reward增加到二三十以上应该自行车就不倒了，记不太清了……\n",
    "for episode in range(100):\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action_size = brain.vector_action_space_size\n",
    "        if brain.vector_action_space_type == 'continuous':\n",
    "            env_info = env.step(np.random.randn(len(env_info.agents), \n",
    "                                                action_size[0]))[default_brain]\n",
    "        else:\n",
    "            action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "            env_info = env.step(action)[default_brain]\n",
    "        episode_rewards += env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
